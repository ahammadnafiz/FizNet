{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "sns.set_palette(\"Spectral\")\n",
    "\n",
    "# Set a base style\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "# Customize specific style parameters\n",
    "custom_params = {\n",
    "    \"axes.spines.right\": False,\n",
    "    \"axes.spines.top\": False,\n",
    "    \"axes.grid\": True,\n",
    "    \"grid.linestyle\": \"--\",\n",
    "    \"grid.color\": \"#cccccc\",\n",
    "    \"axes.facecolor\": \"#f0f0f0\",\n",
    "    \"axes.labelsize\": 14,\n",
    "    \"axes.titlesize\": 16,\n",
    "    \"xtick.labelsize\": 12,\n",
    "    \"ytick.labelsize\": 12\n",
    "}\n",
    "sns.set_context(\"notebook\", rc=custom_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Add the parent directory of 'zylearn' to the Python path\n",
    "sys.path.append(os.path.abspath('..'))  # Adjust the path as necessary\n",
    "\n",
    "# Now import the neural_network module\n",
    "from zylearn.fiznet import neural_network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after iteration 0: 3.378663052191106\n",
      "Cost after iteration 100: 16.448754090700934\n",
      "Cost after iteration 200: 16.80145641310016\n",
      "Cost after iteration 300: 16.28843484292916\n",
      "Cost after iteration 400: 16.416690235471908\n",
      "Cost after iteration 500: 16.641137172421722\n",
      "Cost after iteration 600: 16.67320102055741\n",
      "Cost after iteration 700: 16.33653061513269\n",
      "Cost after iteration 800: 16.192243298522097\n",
      "Cost after iteration 900: 16.14414752631857\n",
      "Cost after iteration 1000: 16.096051754115038\n",
      "Cost after iteration 1100: 16.06398790597935\n",
      "Cost after iteration 1200: 16.176211374454255\n",
      "Cost after iteration 1300: 16.62510524835388\n",
      "Cost after iteration 1400: 16.593041400218194\n",
      "Cost after iteration 1500: 16.384626387336223\n",
      "Cost after iteration 1600: 16.25637099479347\n",
      "Cost after iteration 1700: 16.272402918861317\n",
      "Cost after iteration 1800: 16.272402918861317\n",
      "Cost after iteration 1900: 16.224307146657786\n",
      "Cost after iteration 2000: 16.25637099479347\n",
      "Cost after iteration 2100: 16.11208367818288\n",
      "Cost after iteration 2200: 15.663189804283256\n",
      "Cost after iteration 2300: 16.528913703946817\n",
      "Cost after iteration 2400: 16.625105248353876\n",
      "Fold 1 Validation Accuracy: 0.1076\n",
      "Cost after iteration 0: 16.593041400218194\n",
      "Cost after iteration 100: 16.593041400218194\n",
      "Cost after iteration 200: 16.67320102055741\n",
      "Cost after iteration 300: 16.96177565377859\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 50\u001b[0m\n\u001b[0;32m     47\u001b[0m y_train_fold, y_val_fold \u001b[38;5;241m=\u001b[39m y_train_one_hot[:, train_index], y_train_one_hot[:, val_index]\n\u001b[0;32m     49\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[1;32m---> 50\u001b[0m costs \u001b[38;5;241m=\u001b[39m \u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train_fold\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train_fold\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_iterations\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2500\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meta\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepsilon\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1e-8\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.9\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.999\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlamb\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.01\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbeta3\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.9\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malpha\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.001\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mT_alpha_beta3\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;66;03m# Make predictions for the validation fold\u001b[39;00m\n\u001b[0;32m     53\u001b[0m val_predictions \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mpredict(X_val_fold)\n",
      "File \u001b[1;32md:\\FizNet\\zylearn\\fiznet\\neural_network.py:140\u001b[0m, in \u001b[0;36mNN.train\u001b[1;34m(self, X, Y, num_iterations, eta, epsilon, beta1, beta2, lamb, beta3, alpha, T_alpha_beta3)\u001b[0m\n\u001b[0;32m    137\u001b[0m cost \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_cost(AL, Y)\n\u001b[0;32m    139\u001b[0m \u001b[38;5;66;03m# Backward propagation\u001b[39;00m\n\u001b[1;32m--> 140\u001b[0m grads \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward_propagation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mAL\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaches\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    142\u001b[0m \u001b[38;5;66;03m# Update parameters using the optimizer\u001b[39;00m\n\u001b[0;32m    143\u001b[0m new_theta \u001b[38;5;241m=\u001b[39m optimizer\u001b[38;5;241m.\u001b[39moptimize(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_gradients_vector(grads))\n",
      "File \u001b[1;32md:\\FizNet\\zylearn\\fiznet\\neural_network.py:85\u001b[0m, in \u001b[0;36mNN.backward_propagation\u001b[1;34m(self, AL, Y, caches)\u001b[0m\n\u001b[0;32m     83\u001b[0m     dZ \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m=\u001b[39m D\n\u001b[0;32m     84\u001b[0m     dZ \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkeep_prob\n\u001b[1;32m---> 85\u001b[0m grads[\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdW\u001b[39m\u001b[38;5;132;01m{\u001b[39;00ml\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m1\u001b[39m\u001b[38;5;241m/\u001b[39mm) \u001b[38;5;241m*\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdZ\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mA_prev\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mT\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m+\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlambda_reg \u001b[38;5;241m/\u001b[39m m) \u001b[38;5;241m*\u001b[39m W\n\u001b[0;32m     86\u001b[0m grads[\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdb\u001b[39m\u001b[38;5;132;01m{\u001b[39;00ml\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m1\u001b[39m\u001b[38;5;241m/\u001b[39mm) \u001b[38;5;241m*\u001b[39m np\u001b[38;5;241m.\u001b[39msum(dZ, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, keepdims\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     87\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m l \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\ahamm\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\numpy\\core\\multiarray.py:741\u001b[0m, in \u001b[0;36mdot\u001b[1;34m(a, b, out)\u001b[0m\n\u001b[0;32m    671\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    672\u001b[0m \u001b[38;5;124;03m    result_type(*arrays_and_dtypes)\u001b[39;00m\n\u001b[0;32m    673\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    736\u001b[0m \n\u001b[0;32m    737\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m    738\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m arrays_and_dtypes\n\u001b[1;32m--> 741\u001b[0m \u001b[38;5;129m@array_function_from_c_func_and_dispatcher\u001b[39m(_multiarray_umath\u001b[38;5;241m.\u001b[39mdot)\n\u001b[0;32m    742\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdot\u001b[39m(a, b, out\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m    743\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    744\u001b[0m \u001b[38;5;124;03m    dot(a, b, out=None)\u001b[39;00m\n\u001b[0;32m    745\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    829\u001b[0m \n\u001b[0;32m    830\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m    831\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (a, b, out)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "\n",
    "# Load the digits dataset\n",
    "digits = load_digits()\n",
    "X, y = digits.data, digits.target\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Normalize the data\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Reshape the data to (n_features, n_samples) as required by the NN\n",
    "X_train = X_train.T\n",
    "X_test = X_test.T\n",
    "\n",
    "# One-hot encode the labels\n",
    "def one_hot_encode(y, num_classes):\n",
    "    return np.eye(num_classes)[y].T\n",
    "\n",
    "y_train_one_hot = one_hot_encode(y_train, 10)\n",
    "y_test_one_hot = one_hot_encode(y_test, 10)\n",
    "\n",
    "# Define neural network architecture\n",
    "input_size = X_train.shape[0]\n",
    "hidden_layer1_size = 128\n",
    "hidden_layer2_size = 64\n",
    "num_classes = 10\n",
    "layer_dims = [input_size, hidden_layer1_size, hidden_layer2_size, num_classes]\n",
    "\n",
    "nn = neural_network.NN(layer_dims)\n",
    "# Perform cross-validation\n",
    "kf = KFold(n_splits=5)\n",
    "fold = 1\n",
    "for train_index, val_index in kf.split(X_train.T):\n",
    "    X_train_fold, X_val_fold = X_train[:, train_index], X_train[:, val_index]\n",
    "    y_train_fold, y_val_fold = y_train_one_hot[:, train_index], y_train_one_hot[:, val_index]\n",
    "    \n",
    "    # Train the model\n",
    "    costs = nn.train(X_train_fold, y_train_fold, num_iterations=2500, learning_rate=0.1)\n",
    "    \n",
    "    # Make predictions for the validation fold\n",
    "    val_predictions = nn.predict(X_val_fold)\n",
    "    \n",
    "    # Calculate accuracy for this fold\n",
    "    val_accuracy = accuracy_score(np.argmax(y_val_fold, axis=0), val_predictions)\n",
    "    print(f\"Fold {fold} Validation Accuracy: {val_accuracy:.4f}\")\n",
    "    fold += 1\n",
    "\n",
    "# Final training on the entire training set\n",
    "costs = nn.train(X_train, y_train_one_hot, num_iterations=2500, learning_rate=0.1)\n",
    "\n",
    "# Make predictions for the test set\n",
    "test_predictions = nn.predict(X_test)\n",
    "\n",
    "# Calculate accuracy\n",
    "test_accuracy = accuracy_score(np.argmax(y_test_one_hot, axis=0), test_predictions)\n",
    "print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(np.argmax(y_test_one_hot, axis=0), test_predictions))\n",
    "\n",
    "# Plot the cost over iterations\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.lineplot(x=range(0, 2500, 100), y=costs, marker='o', color='#e74645', linewidth=2)\n",
    "plt.title('Cost vs. Iterations')\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('Cost')\n",
    "plt.show()\n",
    "\n",
    "# Visualize some predictions\n",
    "fig, axes = plt.subplots(2, 5, figsize=(8, 4))\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    ax.imshow(X_test[:, i].reshape(8, 8), cmap='gray')\n",
    "    ax.set_title(f\"Pred: {test_predictions[i]}, True: {np.argmax(y_test_one_hot[:, i])}\")\n",
    "    ax.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
